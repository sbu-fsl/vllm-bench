# vLLM Bench: Real-World LLM Inference Benchmarking for vLLM

## Prereqs

* Kubernetes access via `~/.kube/config` (cluster/context already set).
* `kubectl` (with built-in kustomize).
* Python 3.10+
* Hugging Face token must have access to the chosen model (gated models will 403).

## Dependencies

The `init.sh` script will automatically create a virtual environment (`.venv`) and install the following Python dependencies:

* **requests** - HTTP client for API calls
* **datasets** - HuggingFace Datasets library for loading benchmark datasets
* **rouge-score** - ROUGE metric calculation for summarization tasks
* **tiktoken** - OpenAI tokenizer for token counting
* **huggingface_hub** - HuggingFace Hub client for downloading datasets
* **orjson** - Fast JSON serialization

If you need to install these manually (e.g., for development), run:

```bash
pip install -r requirements.txt
```

### LMCache Support

LMCache is integrated natively with vLLM 0.11.0+ through the `--kv-transfer-config` parameter. No additional packages are required. Simply enable LMCache in your model configuration:

```json
{
  "models": {
    "your-model": {
      "lmcache": {
        "enabled": true,
        "chunk_size": 256,
        "local_cpu": true,
        "max_local_cpu_size": 20,
        "backend": "local"
      }
    }
  }
}
```

The `init.sh` script automatically configures the vLLM server with LMCache when `enabled: true`.

## Files

```
.
├── bench.py                 # Benchmark runner over OpenAI/vLLM APIs
├── config.json              # Single source of truth (namespace, token, model, ports, LMCache)
├── scripts/init.sh          # Generates kustomize, deploys Job, waits, runs bench
├── k8s/                     # Rendered manifests (generated by init.sh)
├── template/                # Base manifests copied into k8s/ by init.sh
├── benchmarks/              # Benchmark task implementations
│   ├── base.py              # Base classes and WorkloadOpts (includes LMCache config)
│   └── ...                  # Individual benchmark implementations
└── runs/                    # Outputs: *.jsonl and *_manifest.json
```

## Configure

Edit `example/config.json` (required keys shown):

```json
{
  "namespace": "usr-xxx-namespace",
  "hf_token": "hf_yyy",
  "data_dir": "./data",
  "models": {
    "facebook/opt-125m": {
      "max_model_len": 1000,
      "endpoint": "http://127.0.0.1:8080",
      "port_local": 8080,
      "port_remote": 8000,
      "api_kind": "completions",
      "max_tokens": 256,
      "temperature": 0.0,
      "top_p": 1.0,
      "qps": 2.0,
      "limit": 2,
      "stream": true,
      "seed": 1234,
      "out_dir": "./runs",
      "model": "facebook/opt-125m",
      "lmcache": {
        "enabled": true,
        "chunk_size": 256,
        "local_cpu": true,
        "max_local_cpu_size": 20,
        "backend": "local"
      }
    }
  },
  "workloads": {}
}
```

Notes:

* `hf_token` is injected as a Secret.
* Each model has its own configuration including LMCache settings.
* If using a gated model (e.g., Meta Llama), ensure the token's account has accepted the license.

### LMCache Configuration

Each model in `config.json` can have an `lmcache` section with the following options:

* `enabled` (boolean): Enable/disable LMCache for this model (default: false)
* `chunk_size` (integer): Token chunk size for cache granularity (default: 256)
* `local_cpu` (boolean): Enable CPU memory offloading (default: true)
* `max_local_cpu_size` (integer): CPU memory limit in GB (default: 20)
* `backend` (string): Storage backend - "local", "redis", etc. (default: "local")

For Redis backend, add:

```json
"lmcache": {
  "enabled": true,
  "backend": "redis",
  "redis_host": "localhost",
  "redis_port": 6379
}
```

## Deploy + Run

```bash
cp example/config.json config.json
./init.sh config.json
```

What it does:

* Generates `k8s/kustomization.yaml` (ConfigMap + Secret).
* Applies manifests (deletes existing Job if needed).
* Waits for the pod to be Ready.
* Port-forwards as per `port_local:port_remote`.
* Runs `bench.py run config.json`.

Outputs land in `./runs/`.

## Change the model or settings

* Edit `example/config.json`.
* Re-run:

  ```bash
  ./init.sh example/config.json
  ```

The script updates the ConfigMap/Secret and restarts the Job only if required (e.g., model changed or pod not Ready).

## Using `bench.py` directly

* List workloads:

  ```bash
  ./bench.py list
  ```
* Run all enabled:

  ```bash
  ./bench.py run config.json --model facebook/opt-125m
  ```
* Run a category/workload:

  ```bash
  ./bench.py run config.json --model facebook/opt-125m spsr alpaca
  ```

## Kubernetes manifests

* Edit base YAMLs under `template/` (e.g., probes, resources, Service).
* `init.sh` copies `template/*` into `k8s/` and injects values from `config.json`.

## Health / Access

* Health: `GET http://127.0.0.1:<port_local>/health`
* If you need manual port-forward via Service:

  ```bash
  kubectl -n <namespace> port-forward service/vllm-service 8080:80
  ```

